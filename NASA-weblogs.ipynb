{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002716.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1619704635879)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@58840c9b\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"NASA web logs\")\n",
    "    .master(\"local\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Comenzamos leyendo los datasets de los weblogs que tenemos como .txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = /Users/mario.serrano/Desktop/NASA/datasets/*.txt\r\n",
       "logsDF: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"/Users/mario.serrano/Desktop/NASA/datasets/*.txt\"\n",
    "val logsDF = spark.read.text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839|\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0                                                   |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0                          |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0                        |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0                           |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "logsDF.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teniendo el dataframe con una sola columna, ahora toca recoger los valores que queremos para nuestra tabla, lo haré mediante expresiones regulares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|host                       |date                |method|resource                                       |protocol|status|bytes|\n",
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|in24.inetnebr.com          |01/Aug/1995:00:00:01|GET   |/shuttle/missions/sts-68/news/sts-68-mcc-05.txt|HTTP/1.0|200   |1839 |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:07|GET   |/                                              |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/ksclogo-medium.gif                     |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/MOSAIC-logosmall.gif                   |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/USA-logosmall.gif                      |HTTP/1.0|304   |0    |\n",
      "|ix-esc-ca2-07.ix.netcom.com|01/Aug/1995:00:00:09|GET   |/images/launch-logo.gif                        |HTTP/1.0|200   |1713 |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:10|GET   |/images/WORLD-logosmall.gif                    |HTTP/1.0|304   |0    |\n",
      "|slppp6.intermind.net       |01/Aug/1995:00:00:10|GET   |/history/skylab/skylab.html                    |HTTP/1.0|200   |1687 |\n",
      "|piweba4y.prodigy.com       |01/Aug/1995:00:00:10|GET   |/images/launchmedium.gif                       |HTTP/1.0|200   |11853|\n",
      "|slppp6.intermind.net       |01/Aug/1995:00:00:11|GET   |/history/skylab/skylab-small.gif               |HTTP/1.0|200   |9202 |\n",
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- resource: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regex: String = (\\S+)\\s(-|\\S+)\\s(-|\\S+)\\s\\[(\\S+)\\s\\-\\d{4}\\]\\s\\\"(GET|POST|PUT|TRACE|HEAD)?\\s*(\\S+|\\S+\\s*\\S*)\\s*(HTTP\\S+\\s*\\S*)?\\\"\\s(\\d{3})\\s(-|\\d+)\r\n",
       "parsedDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val regex = \n",
    "\"\"\"(\\S+)\\s(-|\\S+)\\s(-|\\S+)\\s\\[(\\S+)\\s\\-\\d{4}\\]\\s\\\"(GET|POST|PUT|TRACE|HEAD)?\\s*(\\S+|\\S+\\s*\\S*)\\s*(HTTP\\S+\\s*\\S*)?\\\"\\s(\\d{3})\\s(-|\\d+)\"\"\"\n",
    "\n",
    "val parsedDF = logsDF\n",
    "    .select(regexp_extract($\"value\",regex,1).as(\"host\"),\n",
    "            regexp_extract($\"value\",regex,4).as(\"date\"),\n",
    "            regexp_extract($\"value\",regex,5).as(\"method\"),\n",
    "            regexp_extract($\"value\",regex,6).as(\"resource\"),\n",
    "            regexp_extract($\"value\",regex,7).as(\"protocol\"),\n",
    "            regexp_extract($\"value\",regex,8).cast(\"int\").as(\"status\"),\n",
    "            regexp_extract($\"value\",regex,9).cast(\"int\").as(\"bytes\")\n",
    "           )\n",
    "parsedDF.show(10, false)\n",
    "parsedDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpio los valores nulos para el campo bytes, ya que cuando el status era 404 los bytes recibidos era nulos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n",
       "res3: cleanDF.type = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanDF = parsedDF\n",
    "    .withColumn(\"bytes\", when($\"bytes\".isNull, 0).otherwise($\"bytes\"))\n",
    "cleanDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación para ver que se había realizado bien la organización de los datos, y visto que\n",
    "había filas sin protocolo, comprobar que el resto de datos estaba correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------+\n",
      "|protocol                                         |count  |\n",
      "+-------------------------------------------------+-------+\n",
      "|HTTP/V1.0                                        |279    |\n",
      "|HTTP/1.0From:  <berend@blazemonger.pc.cc.cmu.edu>|1235   |\n",
      "|HTTP/1.0                                         |128    |\n",
      "|HTTP/1.0                                         |3454985|\n",
      "|HTTP/*                                           |13     |\n",
      "|                                                 |4973   |\n",
      "+-------------------------------------------------+-------+\n",
      "\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "|host                   |date                |method|resource                                           |protocol|status|bytes|\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "|pipe1.nyc.pipeline.com |01/Aug/1995:00:12:37|GET   |/history/apollo/apollo-13/apollo-13-patch-small.gif|        |200   |12859|\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:34:55|GET   |/ksc.html                                          |        |200   |7280 |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:00|GET   |/images/ksclogo-medium.gif                         |        |200   |5866 |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:01|GET   |/images/NASA-logosmall.gif                         |        |200   |786  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:02|GET   |/images/MOSAIC-logosmall.gif                       |        |200   |363  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:02|GET   |/images/USA-logosmall.gif                          |        |200   |234  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:03|GET   |/images/WORLD-logosmall.gif                        |        |200   |669  |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:14:40|GET   |/shuttle/countdown/lps/fr.html                     |        |200   |1879 |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:14:51|GET   |/cgi-bin/imagemap/fr                               |        |200   |156  |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:17:34|GET   |/shuttle/countdown/liftoff.html                    |        |200   |5220 |\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test: Unit = ()\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = cleanDF\n",
    "    .groupBy($\"protocol\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"protocol\"))\n",
    "    .show(false)\n",
    "cleanDF.where($\"protocol\" === \"\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*https://medium.com/analytics-vidhya/spark-web-server-logs-analysis-with-scala-74e0ece40a4e\n",
    "\n",
    "val month_map = Map(\"Jan\" -> 1,\"Feb\" -> 2,\"Mar\" -> 3,\"Apr\" -> 4,\"May\" -> 5,\"Jun\" -> 6,\"Jul\" -> 7,\n",
    "                    \"Aug\" -> 8,\"Sep\" -> 9,\"Oct\" -> 10,\"Nov\" -> 11,\"Dec\" -> 12)\n",
    "\n",
    "def parse_clf_time(s: String): String ={\n",
    "    \"%3$s-%2$s-%1$s %4$s:%5$s:%6$s\".format(s.substring(0,2), month_map(s.substring(3,6)),s.substring(7,11),\n",
    "                                          s.substring(12, 14), s.substring(15,17), s.substring(18))\n",
    "}\n",
    "\n",
    "val toTimestamp = udf[String, String](parse_clf_time(_))\n",
    "val logsDF = cleanDF\n",
    "    .withColumn(\"date\",to_timestamp(toTimestamp($\"date\")))\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cambio los valores de los meses del date a forma númerica por si necesitamos en un futuro cambiarlo a un formato de date o timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logsDF: org.apache.spark.sql.DataFrame = [host: string, date: timestamp ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logsDF = cleanDF\n",
    "   .withColumn(\"date\",to_timestamp($\"date\",\"dd/MMM/yyyy:HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ya tendríamos nuestro dataset limpio y listo, para realizar consultas sobre él, en este caso lo guardaremos en parquet para realizar las consultas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsDF.write.format(\"parquet\").mode(\"overwrite\").save(\"/Users/mario.serrano/Desktop/NASA/datasets/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileParquet: String = /Users/mario.serrano/Desktop/NASA/datasets/parquet/*\r\n",
       "parquetDF: org.apache.spark.sql.DataFrame = [host: string, date: timestamp ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileParquet = \"/Users/mario.serrano/Desktop/NASA/datasets/parquet/*\"\n",
    "val parquetDF = spark.read.format(\"parquet\").load(fileParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- resource: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|host                |date               |method|resource                                       |protocol|status|bytes|\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|199.72.81.55        |1995-07-01 00:00:01|GET   |/history/apollo/                               |HTTP/1.0|200   |6245 |\n",
      "|unicomp6.unicomp.net|1995-07-01 00:00:06|GET   |/shuttle/countdown/                            |HTTP/1.0|200   |3985 |\n",
      "|199.120.110.21      |1995-07-01 00:00:09|GET   |/shuttle/missions/sts-73/mission-sts-73.html   |HTTP/1.0|200   |4085 |\n",
      "|burger.letters.com  |1995-07-01 00:00:11|GET   |/shuttle/countdown/liftoff.html                |HTTP/1.0|304   |0    |\n",
      "|199.120.110.21      |1995-07-01 00:00:11|GET   |/shuttle/missions/sts-73/sts-73-patch-small.gif|HTTP/1.0|200   |4179 |\n",
      "|burger.letters.com  |1995-07-01 00:00:12|GET   |/images/NASA-logosmall.gif                     |HTTP/1.0|304   |0    |\n",
      "|burger.letters.com  |1995-07-01 00:00:12|GET   |/shuttle/countdown/video/livevideo.gif         |HTTP/1.0|200   |0    |\n",
      "|205.212.115.106     |1995-07-01 00:00:12|GET   |/shuttle/countdown/countdown.html              |HTTP/1.0|200   |3985 |\n",
      "|d104.aa.net         |1995-07-01 00:00:13|GET   |/shuttle/countdown/                            |HTTP/1.0|200   |3985 |\n",
      "|129.94.144.152      |1995-07-01 00:00:13|GET   |/                                              |HTTP/1.0|200   |7074 |\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF.cache()\n",
    "parquetDF.printSchema\n",
    "parquetDF.show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Dado que podemos encontrar distintos protocolos web, queremos ver cuál es el más utilizado. Para esto agrupa los protocolos y ordenalos de mayor a menor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------+\n",
      "|protocol                                         |count  |\n",
      "+-------------------------------------------------+-------+\n",
      "|HTTP/1.0                                         |3454985|\n",
      "|                                                 |4973   |\n",
      "|HTTP/1.0From:  <berend@blazemonger.pc.cc.cmu.edu>|1235   |\n",
      "|HTTP/V1.0                                        |279    |\n",
      "|HTTP/1.0                                         |128    |\n",
      "|HTTP/*                                           |13     |\n",
      "+-------------------------------------------------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "parquetDF\n",
    "    .groupBy($\"protocol\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que el único protocolo utilizado es el HTTP/1.0, podríamos transformar todos los protocolos a la misma nomenclatura, también nos encontramos que hay varias entradas sin protocolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "protocolDF: org.apache.spark.sql.DataFrame = [host: string, date: timestamp ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val protocolDF = parquetDF\n",
    "    .withColumn(\"protocol\",when($\"protocol\".contains(\"HTTP/\"), \"HTTP/1.0\")\n",
    "                            .when($\"protocol\" === \"\", \"NA\")\n",
    "                            .otherwise($\"protocol\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|protocol|count  |\n",
      "+--------+-------+\n",
      "|HTTP/1.0|3456640|\n",
      "|NA      |4973   |\n",
      "+--------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "protocolDF\n",
    "    .groupBy($\"protocol\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Realiza lo mismo para ver los códigos de estado y los métodos de petición que más se han realizado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|count  |\n",
      "+------+-------+\n",
      "|200   |3100500|\n",
      "|304   |266773 |\n",
      "|302   |73070  |\n",
      "|404   |20866  |\n",
      "|403   |225    |\n",
      "|500   |65     |\n",
      "|null  |60     |\n",
      "|501   |41     |\n",
      "|400   |13     |\n",
      "+------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "parquetDF\n",
    "    .groupBy($\"status\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|method|count  |\n",
      "+------+-------+\n",
      "|GET   |3453403|\n",
      "|HEAD  |7915   |\n",
      "|POST  |222    |\n",
      "|      |73     |\n",
      "+------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "parquetDF\n",
    "    .groupBy($\"method\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto que tenemos 60 entradas con el status en null, comprobamos esos registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------+--------+------+-----+\n",
      "|host|date|method|resource|protocol|status|bytes|\n",
      "+----+----+------+--------+--------+------+-----+\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "|    |null|      |        |        |  null|    0|\n",
      "+----+----+------+--------+--------+------+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "parquetDF.where($\"status\".isNull).show(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que son registros vacíos, por lo que vamos a quitarlos de nuestro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDF: org.apache.spark.sql.DataFrame = [host: string, date: timestamp ... 5 more fields]\r\n",
       "res13: finalDF.type = [host: string, date: timestamp ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalDF = protocolDF.na.drop(Seq(\"status\"))\n",
    "spark.catalog.clearCache()\n",
    "finalDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|count  |\n",
      "+------+-------+\n",
      "|200   |3100500|\n",
      "|304   |266773 |\n",
      "|302   |73070  |\n",
      "|404   |20866  |\n",
      "|403   |225    |\n",
      "|500   |65     |\n",
      "|501   |41     |\n",
      "|400   |13     |\n",
      "+------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "finalDF\n",
    "    .groupBy($\"status\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- También queremos ver cuál es el recurso que tuvo la mayor transferencia de bytes de la página web.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------+---------------------------------------+--------+------+-------+\n",
      "|host |date               |method|resource                               |protocol|status|bytes  |\n",
      "+-----+-------------------+------+---------------------------------------+--------+------+-------+\n",
      "|derec|1995-07-07 14:03:32|GET   |/shuttle/countdown/video/livevideo.jpeg|HTTP/1.0|200   |6823936|\n",
      "+-----+-------------------+------+---------------------------------------+--------+------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "finalDF\n",
    "    .orderBy(desc(\"bytes\"))\n",
    "    .limit(1).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Además queremos saber que recurso de nuestra web es el que más tráfico recibe. Es decir, el recurso con más entradas en nuestro log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+------+\n",
      "|resource                               |count |\n",
      "+---------------------------------------+------+\n",
      "|/images/NASA-logosmall.gif             |208723|\n",
      "|/images/KSC-logosmall.gif              |164976|\n",
      "|/images/MOSAIC-logosmall.gif           |127916|\n",
      "|/images/USA-logosmall.gif              |127082|\n",
      "|/images/WORLD-logosmall.gif            |125933|\n",
      "|/images/ksclogo-medium.gif             |121580|\n",
      "|/ksc.html                              |83912 |\n",
      "|/images/launch-logo.gif                |76009 |\n",
      "|/history/apollo/images/apollo-logo1.gif|68898 |\n",
      "|/shuttle/countdown/                    |64739 |\n",
      "+---------------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "finalDF.groupBy(\"resource\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- También queremos saber que días fueron los que la web recibió más tráfico.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|day|month| count|\n",
      "+---+-----+------+\n",
      "| 13|    7|134201|\n",
      "|  6|    7|100959|\n",
      "|  5|    7| 94573|\n",
      "| 12|    7| 92528|\n",
      "| 31|    8| 90123|\n",
      "|  3|    7| 89584|\n",
      "|  7|    7| 87233|\n",
      "| 14|    7| 84103|\n",
      "| 30|    8| 80640|\n",
      "| 11|    7| 80407|\n",
      "+---+-----+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "finalDF\n",
    "    .withColumn(\"day\",dayofmonth($\"date\"))\n",
    "    .withColumn(\"month\",month($\"date\"))\n",
    "    .groupBy(\"day\",\"month\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Que hosts son los más frecuentes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                host|count|\n",
      "+--------------------+-----+\n",
      "|piweba3y.prodigy.com|21988|\n",
      "|piweba4y.prodigy.com|16437|\n",
      "|piweba1y.prodigy.com|12825|\n",
      "|  edams.ksc.nasa.gov|11964|\n",
      "|        163.206.89.4| 9697|\n",
      "|         news.ti.com| 8161|\n",
      "|www-d1.proxy.aol.com| 8047|\n",
      "|  alyssa.prodigy.com| 8037|\n",
      "| siltb10.orl.mmc.com| 7573|\n",
      "|www-a2.proxy.aol.com| 7516|\n",
      "|www-b2.proxy.aol.com| 7266|\n",
      "|piweba2y.prodigy.com| 7246|\n",
      "|www-b3.proxy.aol.com| 7218|\n",
      "|www-d4.proxy.aol.com| 7211|\n",
      "|www-b5.proxy.aol.com| 7080|\n",
      "|www-d2.proxy.aol.com| 6984|\n",
      "|www-b4.proxy.aol.com| 6972|\n",
      "|www-d3.proxy.aol.com| 6895|\n",
      "|    webgate1.mot.com| 6749|\n",
      "|  e659229.boeing.com| 6720|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "finalDF\n",
    "    .groupBy($\"host\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
