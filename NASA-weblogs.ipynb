{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002716.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1619541490974)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@65dbb9db\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"NASA web logs\")\n",
    "    .master(\"local\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Comenzamos leyendo los datasets de los weblogs que tenemos como .txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = /Users/mario.serrano/Desktop/NASA/datasets/*\r\n",
       "logsDF: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"/Users/mario.serrano/Desktop/NASA/datasets/*\"\n",
    "val logsDF = spark.read.text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839|\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0                                                   |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0                          |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0                        |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0                           |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res1: logsDF.type = [value: string]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsDF.show(5, false)\n",
    "logsDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teniendo el dataframe con una sola columna, ahora toca recoger los valores que queremos para nuestra tabla, lo haré mediante expresiones regulares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|host                       |date                |method|resource                                       |protocol|status|bytes|\n",
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|in24.inetnebr.com          |01/Aug/1995:00:00:01|GET   |/shuttle/missions/sts-68/news/sts-68-mcc-05.txt|HTTP/1.0|200   |1839 |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:07|GET   |/                                              |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/ksclogo-medium.gif                     |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/MOSAIC-logosmall.gif                   |HTTP/1.0|304   |0    |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:08|GET   |/images/USA-logosmall.gif                      |HTTP/1.0|304   |0    |\n",
      "|ix-esc-ca2-07.ix.netcom.com|01/Aug/1995:00:00:09|GET   |/images/launch-logo.gif                        |HTTP/1.0|200   |1713 |\n",
      "|uplherc.upl.com            |01/Aug/1995:00:00:10|GET   |/images/WORLD-logosmall.gif                    |HTTP/1.0|304   |0    |\n",
      "|slppp6.intermind.net       |01/Aug/1995:00:00:10|GET   |/history/skylab/skylab.html                    |HTTP/1.0|200   |1687 |\n",
      "|piweba4y.prodigy.com       |01/Aug/1995:00:00:10|GET   |/images/launchmedium.gif                       |HTTP/1.0|200   |11853|\n",
      "|slppp6.intermind.net       |01/Aug/1995:00:00:11|GET   |/history/skylab/skylab-small.gif               |HTTP/1.0|200   |9202 |\n",
      "+---------------------------+--------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- resource: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parsedDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedDF = logsDF\n",
    "    .select(regexp_extract($\"value\",\"\"\"^([^(\\s)]+)\"\"\",1).as(\"host\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\s-\\s-\\s\\[(\\d{2}\\/\\w{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2})\\s-\\d{4}\\]\"\"\",1).as(\"date\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\s\\\"(\\w*)\"\"\",1).as(\"method\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\s(\\/[^\\\"\\s]*)\"\"\",1).as(\"resource\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\s(HTTP.+\\\"*)\\\"\"\"\",1).as(\"protocol\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\\"\\s([^\\s]*)\"\"\",1).cast(\"int\").as(\"status\"),\n",
    "            regexp_extract($\"value\",\"\"\"^.*\\s([^\\s]*)$\"\"\",1).cast(\"int\").as(\"bytes\")\n",
    "           )\n",
    "parsedDF.show(10, false)\n",
    "parsedDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpio los valores nulos para el campo bytes, ya que cuando el status era 404 los bytes recibidos era nulos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanDF = parsedDF\n",
    "    .withColumn(\"bytes\", when($\"bytes\".isNull, 0).otherwise($\"bytes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobación para ver que se había realizado bien la organización de los datos, y visto que\n",
    "había filas sin protocolo, comprobar que el resto de datos estaba correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------+\n",
      "|protocol                                         |count  |\n",
      "+-------------------------------------------------+-------+\n",
      "|HTTP/V1.0                                        |279    |\n",
      "|HTTP/1.0From:  <berend@blazemonger.pc.cc.cmu.edu>|1235   |\n",
      "|HTTP/1.0                                         |128    |\n",
      "|HTTP/1.0                                         |3455074|\n",
      "|HTTP/*                                           |13     |\n",
      "|                                                 |4884   |\n",
      "+-------------------------------------------------+-------+\n",
      "\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "|host                   |date                |method|resource                                           |protocol|status|bytes|\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "|pipe1.nyc.pipeline.com |01/Aug/1995:00:12:37|GET   |/history/apollo/apollo-13/apollo-13-patch-small.gif|        |200   |12859|\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:34:55|GET   |/ksc.html                                          |        |200   |7280 |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:00|GET   |/images/ksclogo-medium.gif                         |        |200   |5866 |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:01|GET   |/images/NASA-logosmall.gif                         |        |200   |786  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:02|GET   |/images/MOSAIC-logosmall.gif                       |        |200   |363  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:02|GET   |/images/USA-logosmall.gif                          |        |200   |234  |\n",
      "|columbia.acc.brad.ac.uk|01/Aug/1995:00:35:03|GET   |/images/WORLD-logosmall.gif                        |        |200   |669  |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:14:40|GET   |/shuttle/countdown/lps/fr.html                     |        |200   |1879 |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:14:51|GET   |/cgi-bin/imagemap/fr                               |        |200   |156  |\n",
      "|cs1-06.leh.ptd.net     |01/Aug/1995:01:17:34|GET   |/shuttle/countdown/liftoff.html                    |        |200   |5220 |\n",
      "+-----------------------+--------------------+------+---------------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test: Unit = ()\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = cleanDF\n",
    "    .groupBy($\"protocol\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"protocol\"))\n",
    "    .show(false)\n",
    "cleanDF.where($\"protocol\" === \"\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*https://medium.com/analytics-vidhya/spark-web-server-logs-analysis-with-scala-74e0ece40a4e\n",
    "\n",
    "val month_map = Map(\"Jan\" -> 1,\"Feb\" -> 2,\"Mar\" -> 3,\"Apr\" -> 4,\"May\" -> 5,\"Jun\" -> 6,\"Jul\" -> 7,\n",
    "                    \"Aug\" -> 8,\"Sep\" -> 9,\"Oct\" -> 10,\"Nov\" -> 11,\"Dec\" -> 12)\n",
    "\n",
    "def parse_clf_time(s: String): String ={\n",
    "    \"%3$s-%2$s-%1$s %4$s:%5$s:%6$s\".format(s.substring(0,2), month_map(s.substring(3,6)),s.substring(7,11),\n",
    "                                          s.substring(12, 14), s.substring(15,17), s.substring(18))\n",
    "}\n",
    "\n",
    "val toTimestamp = udf[String, String](parse_clf_time(_))\n",
    "val logsDF = cleanDF\n",
    "    .withColumn(\"date\",to_timestamp(toTimestamp($\"date\")))\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cambio los valores de los meses del date a forma númerica por si necesitamos en un futuro cambiarlo a un formato de date o timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logsDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logsDF = cleanDF\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Jan\"), lit(\"01\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Feb\"), lit(\"02\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Mar\"), lit(\"03\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Apr\"), lit(\"04\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"May\"), lit(\"05\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Jun\"), lit(\"06\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Jul\"), lit(\"07\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Aug\"), lit(\"08\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Sep\"), lit(\"09\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Oct\"), lit(\"10\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Nov\"), lit(\"11\")))\n",
    "    .withColumn(\"date\", regexp_replace($\"date\", lit(\"Dec\"), lit(\"12\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ya tendríamos nuestro dataset limpio y listo, para realizar consultas sobre él, en este caso lo guardaremos en parquet para realizar las consultas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsDF.write.format(\"parquet\").mode(\"overwrite\").save(\"/Users/mario.serrano/Desktop/NASA/datasets/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileParquet: String = /Users/mario.serrano/Desktop/NASA/datasets/parquet/*\r\n",
       "parquetDF: org.apache.spark.sql.DataFrame = [host: string, date: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileParquet = \"/Users/mario.serrano/Desktop/NASA/datasets/parquet/*\"\n",
    "val parquetDF = spark.read.format(\"parquet\").load(fileParquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- resource: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|host                |date               |method|resource                                       |protocol|status|bytes|\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "|199.72.81.55        |01/07/1995:00:00:01|GET   |/history/apollo/                               |HTTP/1.0|200   |6245 |\n",
      "|unicomp6.unicomp.net|01/07/1995:00:00:06|GET   |/shuttle/countdown/                            |HTTP/1.0|200   |3985 |\n",
      "|199.120.110.21      |01/07/1995:00:00:09|GET   |/shuttle/missions/sts-73/mission-sts-73.html   |HTTP/1.0|200   |4085 |\n",
      "|burger.letters.com  |01/07/1995:00:00:11|GET   |/shuttle/countdown/liftoff.html                |HTTP/1.0|304   |0    |\n",
      "|199.120.110.21      |01/07/1995:00:00:11|GET   |/shuttle/missions/sts-73/sts-73-patch-small.gif|HTTP/1.0|200   |4179 |\n",
      "|burger.letters.com  |01/07/1995:00:00:12|GET   |/images/NASA-logosmall.gif                     |HTTP/1.0|304   |0    |\n",
      "|burger.letters.com  |01/07/1995:00:00:12|GET   |/shuttle/countdown/video/livevideo.gif         |HTTP/1.0|200   |0    |\n",
      "|205.212.115.106     |01/07/1995:00:00:12|GET   |/shuttle/countdown/countdown.html              |HTTP/1.0|200   |3985 |\n",
      "|d104.aa.net         |01/07/1995:00:00:13|GET   |/shuttle/countdown/                            |HTTP/1.0|200   |3985 |\n",
      "|129.94.144.152      |01/07/1995:00:00:13|GET   |/                                              |HTTP/1.0|200   |7074 |\n",
      "+--------------------+-------------------+------+-----------------------------------------------+--------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF.cache()\n",
    "parquetDF.printSchema\n",
    "parquetDF.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
